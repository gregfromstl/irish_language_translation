{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irish Language Translation: Anything Goes\n",
    "My Anything Goes implementation uses a GRU Encoder-Decoder model with Attention. In addition, I override the model's prediction for a word when that word is likely to not have changed from the source (doesn't change 90% of the time or more). This captures the obvious unchanging words that are characteristic of the dataset while allowing the model to learn how to translate the words that do change.\n",
    "\n",
    "The model itself was influenced largely by tutorials found [here](https://github.com/bentrevett/pytorch-seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filenames and train-validation split. During development I used a standard 70/30 spliot but for the sake of maximizing test performance I've increased the amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lDjkGckEm-_N"
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'train-source': \"train-source.txt\",\n",
    "    'train-target': \"train-target.txt\",\n",
    "    'test-source': \"test-source.txt\",\n",
    "    'test-target': \"test-target.txt\",\n",
    "    'split': 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "59fNjt9FUKi8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the provided filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QvpRH0twnGrI"
   },
   "outputs": [],
   "source": [
    "source = open(PARAMS['train-source'], 'r').read()\n",
    "target = open(PARAMS['train-target'], 'r').read()\n",
    "test_source = open(PARAMS['test-source'], 'r').read()\n",
    "test_target = open(PARAMS['test-target'], 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data. I removed all punctuation under the assumption that it doesn't change during translation, considering most of the changes are simply spelling changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-8B-yNVSnG-s"
   },
   "outputs": [],
   "source": [
    "source = re.sub('\\n', ' ', source)\n",
    "source = re.sub(r'[^\\w\\s<>/]', '', source)\n",
    "target = re.sub('\\n', ' ', target)\n",
    "target = re.sub(r'[^\\w\\s<>/]', '', target)\n",
    "test_source = re.sub('\\n', ' ', test_source)\n",
    "test_source = re.sub(r'[^\\w\\s<>/]', '', test_source)\n",
    "test_target = re.sub('\\n', ' ', test_target)\n",
    "test_target = re.sub(r'[^\\w\\s<>/]', '', test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_sentences` function splits the data on the `<s>` and `</s>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RUMAvk4unHKw"
   },
   "outputs": [],
   "source": [
    "def split_sentences(raw_data: str):\n",
    "    sentences = []\n",
    "    curr = []\n",
    "\n",
    "    for word in raw_data.split(' '):\n",
    "        if word != '<s>' and word != '</s>':\n",
    "            curr.append(word)\n",
    "        if word == '</s>':\n",
    "            sentences.append(' '.join(curr))\n",
    "            curr = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L4ycr74WnHf5"
   },
   "outputs": [],
   "source": [
    "source_data, target_data = split_sentences(source), split_sentences(target)\n",
    "test_source, test_target = split_sentences(test_source), split_sentences(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to a csv file to be read in using torchtext's `TabularDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XoSGleq4nHrO"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'source': source_data, 'target': target_data})\n",
    "test_df = pd.DataFrame({'source': test_source, 'target': test_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vw1PNl8KnOZQ"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "K8eV0qlMUKjK"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare torchtext fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1ozcTKfMUKju"
   },
   "outputs": [],
   "source": [
    "SRC = Field(init_token = '<s>', \n",
    "            eos_token = '</s>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(init_token = '<s>', \n",
    "            eos_token = '</s>', \n",
    "            lower = True)\n",
    "\n",
    "fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the datasets using the saved csv's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0p4VwmXhUKjy"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = TabularDataset('train.csv', 'csv', fields = fields).split(PARAMS['split'])\n",
    "test_dat = TabularDataset('test.csv', 'csv', fields = fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of words that change less than 10% of the time. When constructing translations, I'll override the model's output for these words since they're most likely going to be unchanging. This way, I'll at least get baseline accuracy and can use the ability of the model to understand context for predicting the words that change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_changes = {}\n",
    "for ex in train_data:\n",
    "    for j, source_word in enumerate(ex.src):\n",
    "        if source_word not in source_changes:\n",
    "            source_changes[source_word] = {'exact': 0, 'change': 0}\n",
    "        if source_word in ex.trg:\n",
    "          source_changes[source_word]['exact'] += 1\n",
    "        else:\n",
    "          source_changes[source_word]['change'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_change(change_dict: dict) -> float:\n",
    "  return change_dict['change'] / max(sum(change_dict.values()), 1)\n",
    "unchanging = [word for word in source_changes if calc_change(source_changes[word]) < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12817 unchanging words out of 27913\n"
     ]
    }
   ],
   "source": [
    "print(\"{} unchanging words out of {}\".format(len(unchanging), len(source_changes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum frequency of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VGm99q9iUKj8"
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3dKtj25iUKj_"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Sr9xopOsUKkI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator = BucketIterator(train_data, batch_size = BATCH_SIZE, device = device)\n",
    "valid_iterator = BucketIterator(valid_data, batch_size = BATCH_SIZE, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder uses a single bidirectional GRU with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "p5P1TpS8UKkL"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention module for determining which words to emphasize during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0MD24gGiUKkQ"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder uses context and attention through a single GRU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cCwmx_DlUKkZ"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "    \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model using teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2BU2PiGRUKkd"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a few variations of these hyperparameters. A higher dropout with lower dimensions seems to work the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KpZh83Y1UKkh"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 64\n",
    "DEC_EMB_DIM = 64\n",
    "ENC_HID_DIM = 128\n",
    "DEC_HID_DIM = 128\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = EncoderDecoder(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights with a normal distribution centered at 0 and a standard deviation of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "WKKGJS7-UKkn",
    "outputId": "bd2722cb-6cc3-43d0-8e4e-e5635ce0ec8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(15042, 64)\n",
       "    (rnn): GRU(64, 128, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(13799, 64)\n",
       "    (rnn): GRU(320, 128)\n",
       "    (fc_out): Linear(in_features=448, out_features=13799, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XaAdPLPuUKkv",
    "outputId": "b03c3c47-9ef2-4bf0-b872-c4cac2e3a2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,445,671 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'{count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a few different learning rates. 0.01 won out by far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bkGIUfabUKkz"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare loss and ignore padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wn7frdQXUKla"
   },
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for replacing the models outputs where a word is likely to not have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unchanging(src: list, pred: list) -> list:\n",
    "    output = []\n",
    "    for i, word in enumerate(src):\n",
    "        if word in unchanging:\n",
    "            output.append(word)\n",
    "        else:\n",
    "            output.append(pred[i])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vg82djLtUKlt"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        flattened_output = output[1:].view(-1, output_dim)\n",
    "        flattened_trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(flattened_output, flattened_trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        # this is new to me, I saw it in a tutorial and it seems to work well\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # construct translations for BLEU\n",
    "        pred = torch.argmax(torch.nn.functional.softmax(output, dim=-1), dim=-1)\n",
    "        pred = torch.transpose(pred[1:], 1, 0)\n",
    "        trg = torch.transpose(trg[1:], 1, 0)\n",
    "        src = torch.transpose(src[1:], 1, 0)\n",
    "        for i in range(trg.shape[0]):\n",
    "            vocab_trg = [TRG.vocab.itos[p] for p in trg[i]]\n",
    "            vocab_src = [SRC.vocab.itos[p] for p in src[i]]\n",
    "            end_idx = vocab_trg.index('</s>')\n",
    "            targets.append([vocab_trg[:end_idx]])\n",
    "            outputs.append(check_unchanging(vocab_src[:end_idx], [TRG.vocab.itos[p] for p in pred[i]][:end_idx]))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return epoch_loss / len(iterator), corpus_bleu(targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "j8vl7j3VUKlw"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            flattened_output = output[1:].view(-1, output_dim)\n",
    "            flattened_trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(flattened_output, flattened_trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # construct translations for BLEU\n",
    "            pred = torch.argmax(torch.nn.functional.softmax(output, dim=-1), dim=-1)\n",
    "            pred = torch.transpose(pred[1:], 1, 0)\n",
    "            trg = torch.transpose(trg[1:], 1, 0)\n",
    "            src = torch.transpose(src[1:], 1, 0)\n",
    "            for i in range(trg.shape[0]):\n",
    "                vocab_trg = [TRG.vocab.itos[p] for p in trg[i]]\n",
    "                vocab_src = [SRC.vocab.itos[p] for p in src[i]]\n",
    "                end_idx = vocab_trg.index('</s>')\n",
    "                targets.append([vocab_trg[:end_idx]])\n",
    "                outputs.append(check_unchanging(vocab_src[:end_idx], [TRG.vocab.itos[p] for p in pred[i]][:end_idx]))\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    return epoch_loss / len(iterator), corpus_bleu(targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aeqYjs8bUKmC",
    "outputId": "f2566205-603c-4d0a-f3a5-f7c05faeeea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tTrain Loss: 4.8210 | Train BLEU: 0.4605\n",
      "\t Val. Loss: 3.2811 |  Val. BLEU: 0.5616\n",
      "Epoch 2\n",
      "\tTrain Loss: 3.1644 | Train BLEU: 0.5540\n",
      "\t Val. Loss: 2.8760 |  Val. BLEU: 0.6261\n",
      "Epoch 3\n",
      "\tTrain Loss: 3.2934 | Train BLEU: 0.5677\n",
      "\t Val. Loss: 2.9919 |  Val. BLEU: 0.6607\n",
      "Epoch 4\n",
      "\tTrain Loss: 3.4369 | Train BLEU: 0.5648\n",
      "\t Val. Loss: 3.1967 |  Val. BLEU: 0.6692\n",
      "Epoch 5\n",
      "\tTrain Loss: 3.4357 | Train BLEU: 0.5723\n",
      "\t Val. Loss: 3.1633 |  Val. BLEU: 0.6604\n",
      "Epoch 6\n",
      "\tTrain Loss: 3.3797 | Train BLEU: 0.5702\n",
      "\t Val. Loss: 3.1573 |  Val. BLEU: 0.6620\n",
      "Epoch 7\n",
      "\tTrain Loss: 3.4320 | Train BLEU: 0.5549\n",
      "\t Val. Loss: 3.2786 |  Val. BLEU: 0.6424\n",
      "Epoch 8\n",
      "\tTrain Loss: 4.0764 | Train BLEU: 0.5069\n",
      "\t Val. Loss: 3.7495 |  Val. BLEU: 0.5725\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 8\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_bleu = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_bleu = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f} | Train BLEU: {train_bleu:.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. BLEU: {valid_bleu:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pdKuU_3pUKmR",
    "outputId": "f2fcfae9-5377-4299-803a-4769bc9dcd18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU: 0.6262\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "_, test_bleu = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(\"Test BLEU: {:.4f}\".format(test_bleu))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "3 - Neural Machine Translation by Jointly Learning to Align and Translate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
