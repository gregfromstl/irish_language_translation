{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irish Language Translation: From Scratch\n",
    "Rather than go for a standard EM approach for my From Scratch implementation, I stumbled upon something that's incredibly simple yet works pretty well. In this implementation I take advantage of that facts that:\n",
    "    - the word orderings rarely change from source to target\n",
    "    - a significant percentage of source words remain unchanged in the target\n",
    "    - many of the words that *do* change do so the same way each time\n",
    "These factors would true 100% of the time but they were true often enough I could \"hack\" what I think is a reasonable solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filenames and train-validation split. During development I used a standard 70/30 spliot but for the sake of maximizing test performance I've increased the amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8_82nildV-h3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "PARAMS = {\n",
    "    'train-source': \"train-source.txt\",\n",
    "    'train-target': \"train-target.txt\",\n",
    "    'test-source': \"test-source.txt\",\n",
    "    'test-target': \"test-target.txt\",\n",
    "    'split': 0.9,\n",
    "}\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the provided filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KE19fEcNV3Il"
   },
   "outputs": [],
   "source": [
    "source = open(PARAMS['train-source'], 'r').read()\n",
    "target = open(PARAMS['train-target'], 'r').read()\n",
    "test_source = open(PARAMS['test-source'], 'r').read()\n",
    "test_target = open(PARAMS['test-target'], 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data. I removed all punctuation under the assumption that it doesn't change during translation, considering most of the changes are simply spelling changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G8jp2NlbV80z"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "source = re.sub('\\n', ' ', source)\n",
    "source = re.sub(r'[^\\w\\s<>/]', '', source)\n",
    "target = re.sub('\\n', ' ', target)\n",
    "target = re.sub(r'[^\\w\\s<>/]', '', target)\n",
    "test_source = re.sub('\\n', ' ', test_source)\n",
    "test_source = re.sub(r'[^\\w\\s<>/]', '', test_source)\n",
    "test_target = re.sub('\\n', ' ', test_target)\n",
    "test_target = re.sub(r'[^\\w\\s<>/]', '', test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_sentences` function splits the data on the `<s>` and `</s>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qFOixC_KWEv4"
   },
   "outputs": [],
   "source": [
    "def split_sentences(raw_data: str):\n",
    "    sentences = []\n",
    "    curr = []\n",
    "\n",
    "    for word in raw_data.split(' '):\n",
    "        if word != '<s>' and word != '</s>' and word != '':\n",
    "            curr.append(word.lower())\n",
    "        if word == '</s>':\n",
    "            sentences.append(curr)\n",
    "            curr = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mvRNlSx9WLM9"
   },
   "outputs": [],
   "source": [
    "source_data = split_sentences(source)\n",
    "target_data = split_sentences(target)\n",
    "test_source, test_target = split_sentences(test_source), split_sentences(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train data into train and validation based on the provided ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kNfwp4GJieg2"
   },
   "outputs": [],
   "source": [
    "num_train = int(len(source_data)*PARAMS['split'])\n",
    "train_source, valid_source = source_data[:num_train], source_data[num_train:]\n",
    "train_target, valid_target = target_data[:num_train], target_data[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the vocab for the training data. I could have done this above before splitting the sentences with a simple `set` call, but figured I didn't want to include the start and end tags so this'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r0qpf1FroZeg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(list_of_sents: list) -> set:\n",
    "  vocab = set()\n",
    "  for sentence in list_of_sents:\n",
    "    for word in sentence:\n",
    "      vocab.add(word)\n",
    "  return vocab\n",
    "\n",
    "source_vocab = get_vocab(train_source)\n",
    "target_vocab = get_vocab(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct dictionaries to transition between integer and string representations. This allows me to use a list of lists rather than a dictionary of dictionaries (which eats a ton of RAM) in the below `co_occurrence` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1IM2dNPc5AaS"
   },
   "outputs": [],
   "source": [
    "source_stoi = {src_wrd: i for i, src_wrd in enumerate(source_vocab)}\n",
    "source_itos = {i: src_wrd for i, src_wrd in enumerate(source_vocab)}\n",
    "target_stoi = {trg_wrd: i for i, trg_wrd in enumerate(target_vocab)}\n",
    "target_itos = {i: trg_wrd for i, trg_wrd in enumerate(target_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-occurrence compares each word in the source sentences to the word in the same position in the target sentences. Although occasionally positions won't perfectly match between source and target, they do the majority of the time. So, over the entire training set, two words that are in the same position the most often are likely direct translations of each other. Because many words do not change, these words are often themselves, but they could also be spelling variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CMzKsor0j7Um"
   },
   "outputs": [],
   "source": [
    "co_occurrence = [[0 for trg_word in target_vocab] for src_word in source_vocab]\n",
    "\n",
    "for i, source_sent in enumerate(train_source):\n",
    "  target_sent = train_target[i]\n",
    "  for i, source_word in enumerate(source_sent):\n",
    "    if i < len(target_sent):\n",
    "      target_word = target_sent[i]\n",
    "      co_occurrence[source_stoi[source_word]][target_stoi[target_word]] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `translate` function uses the co-occurrence matrix to predict the most commonly seen target word for each source word. If the word has never been seen, i.e. it's not in the training set, the word is predicted to be unchanging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B7kcSPI5qqJ2"
   },
   "outputs": [],
   "source": [
    "def translate(sentence: list) -> list:\n",
    "  translation = []\n",
    "  for word in sentence:\n",
    "    if word not in source_vocab:\n",
    "      translation.append(word)\n",
    "    else:\n",
    "      word_co = co_occurrence[source_stoi[word]]\n",
    "      translation.append(target_itos[word_co.index(max(word_co))])\n",
    "  return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on some training sentences, seems to be working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "Y6qBmZpw3ja2",
    "outputId": "96216483-d958-4145-9e2a-6fd2308e1621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cinnte', 'go', 'leór', 'thiocfadh', 'dóbhtha', 'bás', 'a', 'fhagháil', 'ar', 'imeall', 'an', 'phuill']\n",
      "['cinnte', 'go', 'leor', 'thiocfadh', 'dóibh', 'bás', 'a', 'fháil', 'ar', 'imeall', 'an', 'phoill']\n",
      "['cinnte', 'go', 'leor', 'thiocfadh', 'dóibh', 'bás', 'a', 'fháil', 'ar', 'imeall', 'an', 'phoill']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['bhí', 'sé', 'follasach', 'go', 'rabh', 'an', 'poll', 'sin', 'ag', 'foscladh', 'ar', 'an']\n",
      "['bhí', 'sé', 'follasach', 'go', 'raibh', 'an', 'poll', 'sin', 'ag', 'foscladh', 'ar', 'an']\n",
      "['bhí', 'sé', 'follasach', 'go', 'raibh', 'an', 'poll', 'sin', 'ag', 'foscladh', 'ar', 'an']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['dfhéadfadh', 'siad', 'bás', 'fhagháil', 'ar', 'a', 'bhruach', 'agus', 'na', 'cuirp', 'imtheacht', 'ar']\n",
      "['dfhéadfadh', 'siad', 'bás', 'a', 'fháil', 'ar', 'a', 'bhruach', 'agus', 'na', 'coirp', 'a']\n",
      "['dfhéadfadh', 'siad', 'bás', 'fháil', 'ar', 'a', 'bhruach', 'agus', 'na', 'coirp', 'imeacht', 'ar']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['thiocfadh', 'dóbhtha', 'fosta', 'lámh', 'a', 'chur', 'ina', 'mbás', 'féin', 'a', 'ghabháil', 'de']\n",
      "['thiocfadh', 'dóibh', 'fosta', 'lámh', 'a', 'chur', 'ina', 'mbás', 'féin', 'a', 'ghabháil', 'de']\n",
      "['thiocfadh', 'dóibh', 'fosta', 'lámh', 'a', 'chur', 'ina', 'mbás', 'féin', 'a', 'ghabháil', 'de']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['na', 'dhiaidh', 'sin', 'bhí', 'rud', 'éigin', 'dochreidte', 'agus', 'leamh', 'in', 'gach', 'teóir']\n",
      "['ina', 'dhiaidh', 'sin', 'bhí', 'rud', 'éigin', 'dochreidte', 'agus', 'leamh', 'i', 'ngach', 'teoiric']\n",
      "['na', 'dhiaidh', 'sin', 'bhí', 'rud', 'éigin', 'dochreidte', 'agus', 'leamh', 'i', 'gach', 'teoiricí']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(train_source[i][:12])\n",
    "  print(train_target[i][:12])\n",
    "  print(translate(train_source[i][:12]))\n",
    "  print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my attempt at manually calculating BLEU. While it was fairly close to the nltk calculations, it was off by a little so below I'm using the nltk library for the sake of making sure my results are as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NhYltAKIEJiX"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ngrams(sent: list, n: int):\n",
    "  ngrams = {}\n",
    "  for i in range(len(sent)-n+1):\n",
    "    curr_ngram = \" \".join(sent[i:i+n])\n",
    "    if curr_ngram not in ngrams:\n",
    "      ngrams[curr_ngram] = 1\n",
    "    else:\n",
    "      ngrams[curr_ngram] += 1\n",
    "  return ngrams\n",
    "\n",
    "def calc_bleu(target: list, pred: list) -> float:\n",
    "  n = min(4, len(pred))\n",
    "  all_ngrams = 1\n",
    "  for i in range(n):\n",
    "    count = ngrams(pred, i+1)\n",
    "    clip = ngrams(target, i+1)\n",
    "    clip = {k: clip[k] for k in clip if k in count}\n",
    "    all_ngrams *= sum(clip.values()) / sum(count.values())\n",
    "  brevity = math.exp(1-len(pred)/len(target)) if len(pred) < len(target) else 1\n",
    "  return (all_ngrams**(1/n)) * brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test` function simply calculates BLEU over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OeGSZjgg75RI"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def test(source: list, targets: list) -> float:\n",
    "  running_bleu = 0\n",
    "  nltk_bleu = 0\n",
    "\n",
    "  pred_targets = []\n",
    "  true_targets = []\n",
    "  for i, sentence in enumerate(source):\n",
    "    running_bleu += sentence_bleu([targets[i]], translate(sentence))\n",
    "\n",
    "  return running_bleu / i\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HATArDZ9DTzT",
    "outputId": "a18fd0c7-3f2c-466e-c6f6-37892ca456be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BLEU: 0.7782\n",
      "Val. BLEU: 0.6920\n"
     ]
    }
   ],
   "source": [
    "print(\"Train BLEU: {:.4f}\".format(test(train_source, train_target)))\n",
    "print(\"Val. BLEU: {:.4f}\".format(test(valid_source, valid_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "jStyUjF1av1W",
    "outputId": "2827bd68-57c3-40b7-9ee7-c41dfd28ff25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bhéarfadh', 'sin', 'faoiseamh', 'intinne', 'dó']\n",
      "['bhéarfadh', 'sin', 'faoiseamh', 'intinne', 'dó']\n",
      "['bhéarfadh', 'sin', 'faoiseamh', 'intinne', 'dó']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['nó', 'bhí', 'cineál', 'de', 'scrupall', 'coinnsíosa', 'i', 'rith', 'an', 'ama', 'air', 'nach']\n",
      "['nó', 'bhí', 'cineál', 'de', 'scrupall', 'coinsiasa', 'i', 'rith', 'an', 'ama', 'air', 'nach']\n",
      "['nó', 'bhí', 'cineál', 'de', 'scrupall', 'coinnsíosa', 'i', 'rith', 'an', 'ama', 'air', 'nach']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['cuireadh', 'an', 'bád', 'ar', 'an', 'tuinn', 'eadar', 'sin', 'is', 'tráthas', 'agus', 'dimthigh']\n",
      "['cuireadh', 'an', 'bád', 'ar', 'an', 'toinn', 'eadar', 'sin', 'is', 'tráthas', 'agus', 'dimigh']\n",
      "['cuireadh', 'an', 'bád', 'ar', 'an', 'toinn', 'idir', 'sin', 'is', 'tráthas', 'agus', 'dimigh']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['ní', 'thiocfaidh', 'trí', 'lá', 'go', 'rabh', 'sí', 'arais', 'i', 'gcaslaigh', 'rann', 'na']\n",
      "['ní', 'thiocfaidh', 'trí', 'lá', 'go', 'raibh', 'sí', 'ar', 'ais', 'i', 'gcaslaigh', 'rinn']\n",
      "['ní', 'thiocfaidh', 'trí', 'lá', 'go', 'raibh', 'sí', 'ar', 'i', 'gcaslaigh', 'rann', 'na']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['ní', 'hé', 'sin', 'a', 'shíleas', 'tuathal', 'eoghain', 'arsa', 'duine', 'eile']\n",
      "['ní', 'hé', 'sin', 'a', 'shíleas', 'tuathal', 'eoghain', 'arsa', 'duine', 'eile']\n",
      "['ní', 'é', 'sin', 'a', 'shíleas', 'tuathal', 'eoghain', 'arsa', 'duine', 'eile']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(valid_source[i][:12])\n",
    "  print(valid_target[i][:12])\n",
    "  print(translate(valid_source[i][:12]))\n",
    "  print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zUydwD8UfSOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BLEU: 0.7734\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing BLEU: {:.4f}\".format(test(test_source, test_target)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "From Scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
